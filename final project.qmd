---
title: "draft3"
format: html
editor: visual
---

# Predicting Music Genre

## Using Machine Learning Models and Spotify Data to Predict the Genre of the Song

### Ashley Son

## UCSB Winter 2023

# Introduction

![](tumblr_cf911986ed439c4edbda3e97cd4a1759_63b96dd5_1280.gif)

![](images/spotiify-01.gif)

The aim of this project is to build a machine learning model that can predict the genre of a song.

The data set I will be using is from Spotify via the **spotifyr** package. This data set is pulled from the Spotify's API containing various songs of various genres. I will be using this to implement multiple techniques to yield the most accurate model for this multiclass classification problem.

## What is Music Genre?

Music genre is a way of categorizing music based on its style, structure, and cultural influences. Some popular music genres include pop, rock, hip hop, jazz, blues, and country. Each genre has its own distinctive sound, rhythm, and instrumentation.

## Why?

Predicting the genre of a song can be useful for music lovers who are interested in discovering new music. With the large and constantly growing number of songs available, it can be difficult for listeners to find new music that matches their taste. A genre prediction model can help users to filter songs based on their preferred genre, making it easier for them to discover new music. Additionally, the model can be useful for music recommendation systems, which can suggest new songs based on a user's listening history and preferences.

# Exploratory Data Analysis

## Loading Packages and Data

Lets load in all the required packages and raw data set for this project.

```{r}
require(tidyverse)
require(tidymodels)
require(spotifyr)
require(dplyr)
require(tidymodels)
require(readr)
require(kknn)
require(ISLR)
require(discrim)
require(poissonreg)
require(glmnet)
require(corrr)
require(corrplot)
require(kernlab)
require(randomForest)
require(xgboost)
require(formattable)
require(rpart.plot)
require(vip)
require(ranger)
require(ggplot2)
require(rsample)
require(tune)
tidymodels::tidymodels_prefer()
```

```{r}
library(readr)
#playlist_songs <- read_csv("Desktop/genre_songs.csv")
playlist_songs <- genre_songs
feature_names <- names(playlist_songs)[12:23]
glimpse(playlist_songs, width = 60)
```

## Describing the Predictors

| variable                 | class     | description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
|-------------------|-------------------|------------------------------------|
| track_id                 | character | Song unique ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| track_name               | character | Song Name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| track_artist             | character | Song Artist                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| track_popularity         | double    | Song Popularity (0-100) where higher is better                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| track_album_id           | character | Album unique ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| track_album_name         | character | Song album name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| track_album_release_date | character | Date when album released                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| playlist_name            | character | Name of playlist                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| playlist_id              | character | Playlist ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| playlist_genre           | character | Playlist genre                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| playlist_subgenre        | character | Playlist subgenre                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| danceability             | double    | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.                                                                                                                                                                                                                                                                       |
| energy                   | double    | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.                                                                                                                          |
| key                      | double    | The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.                                                                                                                                                                                                                                                                                                                            |
| loudness                 | double    | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.                                                                                                                                                                                       |
| mode                     | double    | Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.                                                                                                                                                                                                                                                                                                                                                    |
| speechiness              | double    | Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |
| acousticness             | double    | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.                                                                                                                                                                                                                                                                                                                                                                                       |
| instrumentalness         | double    | Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.                                                                                                                 |
| liveness                 | double    | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.                                                                                                                                                                                                                                                                                            |
| valence                  | double    | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).                                                                                                                                                                                                                                                                  |
| tempo                    | double    | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.                                                                                                                                                                                                                                                                                                                         |
| duration_ms              | double    | Duration of song in milliseconds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |

## Tidying the Raw Data

Lets take a look at the data set.

```{r}
head(playlist_songs)
dim(playlist_songs)
colnames(playlist_songs)
```

Looking at the dimensions and column names of our data set, it looks like there are 31,390 observations and 23 variables. We need to make sure that there are no NA values that might interfere with our data analysis and reduce the accuracy of descriptive statistics.

```{r}
sum(is.na(playlist_songs) == "TRUE")
which(is.na(playlist_songs), arr.ind = TRUE)
```

Looks like there are no NA values. Great! We also must check if there are any songs shorter than 4 seconds and remove those.

```{r}
playlist_songs <- filter(playlist_songs, playlist_songs$duration_ms>4000)
```

Now that our raw data is tidy, lets move on to exploring the different audio features by genre.

### Density

```{r}
playlist_songs %>%
  select(c('playlist_genre', feature_names)) %>%
  pivot_longer(cols = feature_names) %>%
  ggplot(aes(x = value)) +
  geom_density(aes(color = playlist_genre), alpha = 0.5) +
  facet_wrap(~name, ncol = 3, scales = 'free') +
  labs(title = 'Spotify Audio Feature Density - by Genre',
       x = '', y = 'density') +
  theme(axis.text.y = element_blank()) 
```

**Overall, the songs in the data set tend to have low acousticness, liveness, instrumentalness, and speechiness, with higher danceability, energy, and loudness. Valence varies across genres.**

**Breaking things out by genre, 'EDM' tracks are least likely to be acoustic and most likely to have high energy with low valence (sad or depressed)**

**'latin' tracks have high valence (are positive and cheerful) with danceability**

**'rap' songs score highly for speechiness and danceability**

**'rock' songs are most likely to be recorded live and have low danceability**

**'pop', 'latin', and 'EDM' songs are more likely to have shorter durations compared to 'R&B', 'rap', and 'rock'**

**Based on the density plot, it looks like energy, valence, tempo, and danceability may provide the most seperation between genres during classification, while instrumentalness and key may not help so much**

### Outliers

Some of the data points have durations that are much longer or shorter than the rest, which could affect our analysis. To address this, we will exclude these unusual values from our dataset and analyze the remaining data. To identify these outliers, we will use the 'boxplot' function, which highlights any data points that are outside of a certain range. By default, this range is the interquartile range, which covers the middle 50% of the data. However, since many of our data points fall outside of this range, we will increase the range parameter to '5', which expands the range by four times the interquartile range. This will allow us to identify a broader set of data points that we consider to be outliers.

```{r}
with_outliers <- playlist_songs %>%
  ggplot(aes(y = duration_ms)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = 'Duration') 
duration_outliers <- boxplot(playlist_songs$duration_ms, 
                             plot = FALSE, range = 5)$out
playlist_songs_no_outliers <- playlist_songs %>%
  filter(!duration_ms %in% duration_outliers) 
without_outliers <- playlist_songs_no_outliers %>%
  ggplot(aes(y = duration_ms)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = 'Duration, outliers removed') 
gridExtra::grid.arrange(with_outliers, without_outliers, ncol = 1)
```

After identifying and removing outliers, we were left with a dataset containing 131 songs that were considered to be unusually long or short in duration. As a result of removing these outliers, the maximum duration in the dataset was 516,000 milliseconds, or 8.6 minutes, compared to the original maximum of 5,100,000 milliseconds, or 85 minutes.

### Correlations

We will now take a look at the correlations between the different features and their correlations.

```{r}
playlist_songs_no_outliers %>%
  select(feature_names) %>%
  scale() %>%
  cor() %>%
  corrplot::corrplot(method = 'color', 
                     order = 'hclust', 
                     type = 'upper', 
                     diag = FALSE, 
                     tl.col = 'black',
                     addCoef.col = "grey30",
                     number.cex = 0.6,
                     main = 'Audio Feature Correlation',
                     mar = c(2,2,2,2),
                     family = 'Avenir')
```

In the dataset comprising various songs and genres, energy and loudness have a strong positive correlation of 0.68. However, we decided to remove the loudness attribute since energy showed more significant differences between different genre groups, as indicated by the density plot.

In addition, we observed a negative correlation between energy and acousticness, which is logical. We also found a positive correlation between danceability and valence, meaning that happier songs are more likely to make people dance. Liveness, tempo, and energy appear to be closely related, as are speechiness and danceability. Interestingly, we found a negative correlation between danceability and both tempo and energy.

We will now remove loudness.

```{R}
feature_names_reduced <- names(playlist_songs)[c(12:14,16:23)]
```

We will now find the median feature value for each genre to compute the correlations to see if there are any similarities between.

```{r}
# average features by genre
avg_genre_matrix <- playlist_songs_no_outliers %>%
  group_by(playlist_genre) %>%
  summarise_if(is.numeric, median, na.rm = TRUE) %>%
  ungroup() 
avg_genre_cor <- avg_genre_matrix %>%
  select(feature_names_reduced, -mode) %>% 
  scale() %>%
  t() %>%
  as.matrix() %>%
  cor() 
colnames(avg_genre_cor) <- avg_genre_matrix$playlist_genre
row.names(avg_genre_cor) <- avg_genre_matrix$playlist_genre
avg_genre_cor %>% corrplot::corrplot(method = 'color', 
                     order = 'hclust',
                     type = 'upper',
                     tl.col = 'black',
                     diag = FALSE,
                     addCoef.col = "grey40",
                     number.cex = 0.75,
                     mar = c(2,2,2,2),
                     main = 'Correlation Between Median Genre Feature Values',
                     family = 'Avenir')
```

We can see that **`R&B`** and **`EDM`** genres have a negative correlation with all other genres in the dataset, except for each other, which suggests that they are relatively distinct from other genres but not from each other.

Among all genres, **`Latin`** and **`R&B`** are the most similar, with a positive correlation of 0.57, indicating that they have similar median feature values. In contrast, **`EDM`** and **`R&B`**, as well as **`EDM`** and **`Latin`**, have the largest differences among all pairs of genres, with negative correlations of -0.83 and -0.69, respectively.

# Setting up Models

We will now set up our models. We will perform our train/test split, create our recipe, and establish cross validation to help with our models.

## Preparing the data for training

```{r}
playlist <- playlist_songs_no_outliers %>%
  mutate_if(is.numeric, scale)
playlist$mode <- as.factor(playlist$mode)
playlist$key <- as.factor(playlist$key)
```

## Train/Test Split

```{r}
set.seed(1234)
playlist_split <- initial_split(playlist, prop = 0.75,
                               strata = playlist_genre)
playlist_train <- training(playlist_split)
playlist_test <- testing(playlist_split) 
```

```{r}
dim(playlist_test)
```

```{r}
dim(playlist_train)
```

Currently, the training dataset contains 23,442 observations, while the testing dataset contains 7,817 observations. These values are sufficient for building an efficient model.

## Building Our Recipe

As we will be utilizing identical predictors, response variable, and model conditions, we plan to create a single recipe that all our models will use. The recipe will incorporate 11 predictor variables, namely, danceability, energy, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, and duration_ms. We will exclude any ID variables used to identify individual songs since they are not relevant for predicting playlist_genre.

We will treat mode and key as categorical variables and convert them into dummy variables. Additionally, we will standardize our data by centering and scaling it to optimize model performance. It is worth noting that identifying the individual songs has no bearing on the model's accuracy.

```{r}
music_recipe <- recipe(playlist_genre ~ danceability + energy + key + mode + speechiness + acousticness + instrumentalness + liveness + valence + tempo + duration_ms ,
                         data = playlist_train) %>%
  step_naomit(all_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  step_center(all_numeric_predictors()) %>%
  step_dummy(mode) %>%
  step_dummy(key)
```

## K-Fold Cross Validation

To address the issue of imbalanced data, we will employ a technique called stratified cross-validation. Our cross-validation strategy will be stratified on the response variable, `playlist_genre`.

```{r}
playlist_folds <- vfold_cv(playlist_train, v = 10, strata = playlist_genre)
```

```{r}
save(playlist_folds, music_recipe, playlist_train, playlist_test, file = "/Users/ashleyson/Desktop/Playlist-Modeling-Setup.rda")
```

# Model Building

We will proceed with building our models using the recipe that we initialized earlier. Given the varying computational times for these models, I have created a separate qmd file, which can be accessed on my GitHub page at [**https://github.com/ashleyson/Music-Genre-Classification/tree/main/models**](https://github.com/ashleyson/Music-Genre-Classification/tree/main/models) within the models folder.

In order to evaluate the performance of our models, I have decided to choose the roc_auc metric as our primary performance metric. This metric provides a comprehensive assessment of the model's ability to distinguish between different classes, and is particularly suitable for multi-class classification problems.

By computing the area under the ROC curve, which plots the true positive rate against the false positive rate for various classification thresholds, the roc_auc metric gives an overall measure of the model's performance across all possible threshold values. This makes it a robust and informative metric for assessing the model's performance, and allows us to obtain a clear understanding of how well the model is performing on the entire range of possible classification thresholds.

To set up our models, we will specify the type of model, set its engine, and set its mode as classification. We will then establish our workflow, add the new model, and include the recipe that we have established.

We will set up 4 models: Decision Tree, Random Forest, QDA, and LASSO regression models, using the recipe we have made

To fine-tune our models, we will set up a tuning grid with the parameters that we want to be tuned, as well as the number of different levels of tuning. We will then tune the model using our selected parameters. Once we have selected the most accurate model from all the tuning, we will finalize our workflow with those tuning parameters.

Next, we will fit our model to the training dataset and save the results to an RDA file to avoid having to run the same analysis repeatedly.

# Performance Metrics

In order to evaluate the performance of our model, we will be utilizing two essential metrics: accuracy and roc_auc. While both metrics provide valuable insights, we will mainly focus on the roc_auc metric due to its effectiveness in evaluating classification models that have imbalanced data.

The roc_auc metric measures the area under the curve (AUC) for the receiver operating characteristic (ROC) curve, which is a graphical representation that illustrates the ability of a binary classifier system to distinguish between positive and negative classes as the decision-making threshold changes. This metric provides a more robust evaluation of the model's performance across different classification thresholds, especially in cases where the data is not perfectly balanced.

Although accuracy is a useful metric, it may not provide a complete evaluation of the model's performance, particularly in cases where the data is imbalanced. By focusing on the roc_auc metric, we can obtain a more comprehensive understanding of the model's ability to classify instances, and make informed decisions regarding the model's overall performance.

As stated earlier, we fit four different models. These models were Quadratic Discriminant Analysis, Lasso, Decision Tree, Random Forest, and Support Vector Machine models.

LASSO

#0.45670 accuracy

\# 0.7800820 ROC_AUC

DEC TREE

#0.460455 accuracy

#0.7684563 ROC_AUC

QDA

#0.4231 accuracy

#0.7568136 ROC_AUC

RF

#0.5145 accuracy

#0.8142305 ROC_AUC

```{r}
load("/Users/ashleyson/Desktop/Music-Genre-Classification/RDA/Playlist-Modeling-Setup.rda")
load("/Users/ashleyson/Desktop/Music-Genre-Classification/RDA/Playlist_Random_Forest.rda")
load("/Users/ashleyson/Desktop/Music-Genre-Classification/RDA/Playlist-DecTree-Model.rda")
load("/Users/ashleyson/Desktop/Music-Genre-Classification/RDA/Playlist_QDA.rda")
load("/Users/ashleyson/Desktop/Music-Genre-Classification/RDA/Playlist_LASSO.rda")
```

## Visualizing our Results

The autoplot function in R is an invaluable tool for visualizing the results of tuned models. By utilizing this function, we can visualize the impact of changes in various parameters on our chosen metric, which in this case is roc_auc.

To conserve space, we will only display the plots of models that have the three highest ROC AUC values in the upcoming section. This will allow us to explore and showcase the most promising models in a more detailed manner.

### Random Forest

The random forest model is a powerful supervised ensemble learning technique that comprises of multiple decision trees. While decision tree models may often overfit to the training data, the random forest algorithm overcomes this limitation by averaging the prediction results of each decision tree and determining a final output.

By stacking multiple classifiers together, the random forest algorithm improves its overall performance and generates more accurate predictions. This technique ensures that the model is more robust and less prone to errors, making it an effective tool for various machine learning tasks.

```{r}
autoplot(playlist_rf_tune_res_auc)
```

### Decision Tree

The decision tree algorithm is a popular supervised learning technique that creates a model in the form of a tree structure. While decision trees are intuitive and easy to understand, they can be prone to overfitting the training data and may not generalize well to new data.

To mitigate these issues, techniques such as pruning and setting constraints on the maximum depth of the tree can be employed. Additionally, ensemble learning techniques such as random forests can also be used to improve the performance of decision tree models by combining multiple decision trees together.

By using decision tree algorithms and implementing these techniques, we can build robust models that are capable of accurately predicting outcomes for a variety of machine learning problems.

```{r}
autoplot(playlist_dt_tune_res)
```

### LASSO Regression

LASSO Regression is a powerful regularization technique that can be used to reduce the complexity of linear regression models by shrinking the coefficients of less important features to zero. This helps to prevent overfitting and improve the generalization performance of the model.

By introducing a penalty term based on the L1-norm of the coefficients, the LASSO Regression algorithm can effectively select the most relevant features and discard the irrelevant ones. This makes the model more interpretable and reduces the risk of overfitting, particularly when dealing with high-dimensional datasets.

LASSO Regression is widely used in a variety of machine learning applications, particularly when dealing with sparse data or when the number of features is large relative to the number of observations. By incorporating LASSO Regression into our machine learning pipeline, we can build more accurate and robust models that can effectively capture the underlying patterns in the data.

```{r}
autoplot(playlist_lasso_tune_res)
```

# Results from our Best Models

Now that we know that our best model was a random forest, we can progress to analyzing its true results.

## Random Forest Model

Now that we have determined that the random forest model had the best overall performance, we need to assess its ability to generalize to new and unseen data. While the high ROC AUC scores observed during the training phase indicate the model's strong performance on the same data it was trained on, it is important to verify if the model can maintain this performance on new and unseen data.

### And the Best Model is...

Random Forest 444! After evaluating the performance of different prediction models, we have determined that Random Forest model #444, performed the best overall among all the random forest models, and even outperformed the other three different prediction models. The model's output and scores, along with its associated parameters, are provided below.tput and scores, as well as the its associated parameters.

```{r}
show_best(playlist_rf_tune_res_auc, metric = "roc_auc") %>%
  select(-.estimator, .config) %>%
  slice(1)
```

Now that we have our best overall model, we can finally fit it to our testing data and discover its actual performance in predicting the song's genre!

## Final ROC AUC Results

```{r}
# use `finalize_workflow()` and `fit()` to fit the model to the training set

final_rf_model_test <- augment(playlist_rf_final_fit_auc , 
                                playlist_test) %>% 
  select(playlist_genre, starts_with(".pred"))


roc_data <- final_rf_model_test %>% 
  dplyr::select(starts_with(".pred_"))

roc_curve(roc_data, truth = .pred_class, .pred_edm, .pred_latin, .pred_pop, .pred_rap, 
          `.pred_r&b`, .pred_rock) %>% 
  # Plot the ROC curve
  autoplot()

```
